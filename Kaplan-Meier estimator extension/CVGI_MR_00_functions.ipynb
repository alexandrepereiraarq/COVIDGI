{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>imports to environment</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "#from zipfile import ZipFile\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "from lifelines.plotting import plot_lifetimes      # Lifeline package for the Survival Analysis\n",
    "%pylab inline\n",
    "from lifelines import KaplanMeierFitter\n",
    "kmf = KaplanMeierFitter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> import dataframes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating functions that search parts of strings and clip values\n",
    "def left(aString, howMany):\n",
    "    if howMany <1:\n",
    "        return ''\n",
    "    else:\n",
    "        return aString[:howMany]\n",
    "\n",
    "def right(aString, howMany):\n",
    "    if howMany <1:\n",
    "        return ''\n",
    "    else:\n",
    "        return aString[-howMany:]\n",
    "\n",
    "def mid(aString, startChar, howMany):\n",
    "    if howMany < 1:\n",
    "        return ''\n",
    "    else:\n",
    "        return aString[startChar:startChar+howMany]\n",
    "\n",
    "def new_columns(df, columnsList):\n",
    "    df = df.join(pd.DataFrame(str(np.nan), index=df.index, dtype='str',columns=columnsList))\n",
    "    return df\n",
    "\n",
    "def print_header(text_in):\n",
    "    text_top_bottom = \"\\n========================================================================\"\n",
    "    len_mid = np.floor((len(text_top_bottom) - len(text_in))/2)\n",
    "    text_mid_a = \"\"\n",
    "    for n in range(int(len_mid)):\n",
    "        text_mid_a = text_mid_a + \"+\"\n",
    "    text_mid = \"\\n\"+text_mid_a + text_in + text_mid_a\n",
    "    while len(text_top_bottom) < len(text_mid):\n",
    "        text_top_bottom = text_top_bottom + \"=\"\n",
    "    text_out = text_top_bottom + text_mid + text_top_bottom + \"\\n\"\n",
    "    return text_out\n",
    "\n",
    "\n",
    "def imports():\n",
    "    global file_path\n",
    "    global randSample\n",
    "    #randSample = sorted(sample(range(5000),k=25))\n",
    "    search_select = input(\"What is the source of the MOVEMENT data? (1=internet, 2=home Mac, 3=office Windows)\")\n",
    "    import_type = input(\"What type of import is it? (1 = initial (only cases and movement), 2 = initial (full), 3 = data by week, 4 = merged datasets, 5 = merged and cleaned)\")    \n",
    "    file_path = \"\"\n",
    "    print(file_path)\n",
    "    if search_select == \"2\":\n",
    "        file_path = Path('../../../02 data/')\n",
    "        print(file_path)\n",
    "    elif search_select == \"3\":\n",
    "        file_path = Path('./IN/')\n",
    "        print(file_path)\n",
    "    \n",
    "    if import_type == \"1\":\n",
    "        import_movement_bra()\n",
    "        import_brasilio()\n",
    "    \n",
    "    elif import_type == \"2\":\n",
    "        import_movement()\n",
    "        import_brasilio()\n",
    "        import_responses_souza()\n",
    "        import_responses_barberia()\n",
    "        import_recoded()\n",
    "        import_ibge_codes()\n",
    "    \n",
    "    elif import_type == \"3\":\n",
    "        import_recoded()\n",
    "        import_ibge_codes()\n",
    "    \n",
    "    elif import_type == \"4\":\n",
    "        import_merged()\n",
    "        import_ibge_codes()\n",
    "    elif import_type == \"5\":\n",
    "        import_cleaned()\n",
    "        #import_ibge_codes()        \n",
    "\n",
    "def import_movement():\n",
    "    global movement\n",
    "    global search_select\n",
    "    #loading Movement Range Maps world data for a given date \n",
    "    file_name = \"WD FACEBOOK Data for Good/movement-range-2021-04-11.csv\"\n",
    "    file_url = file_path / file_name\n",
    "    movement = pd.read_csv(file_url,delimiter='\\t', encoding='utf-8',parse_dates=['ds'])\n",
    "    print(print_header(\"Movement Range\"), movement.info())\n",
    "    \n",
    "def import_brasilio():\n",
    "    #loading Brasil.io daily cases\n",
    "    global cases\n",
    "    global deaths\n",
    "    file_name = \"BR BRASIL.IO/2021-05-25 caso_full.csv\"\n",
    "    file_url = file_path / file_name\n",
    "    print(file_url)\n",
    "    cases = pd.read_csv(file_url,parse_dates=['date'])\n",
    "    print(print_header(\"Daily Cases (Brasil.io)\"), cases.info())\n",
    "\n",
    "def import_movement_bra():\n",
    "    global movement_bra\n",
    "    file_name = \"WD FACEBOOK Data for Good/movement-range-2021-04-11_BRA.csv\"\n",
    "    file_url = file_path / file_name\n",
    "    movement_bra = pd.read_csv(file_url)\n",
    "    print(print_header(\"Movement Range\"),movement_bra.info())\n",
    "    \n",
    "def import_responses_souza():\n",
    "    global responses_souza\n",
    "    file_name = \"BR SOUZA et al Non-Pharmaceutical Interventions in BR/Dataset_Port_and_Eng.xlsx\"\n",
    "    file_url = file_path / file_name\n",
    "    responses_souza = pd.read_excel(file_url, sheet_name=\"Covid-19 NPIs (english)\", parse_dates=['Q1. Start date','Q2. Start date', 'Q3. Start date', 'Q4. Start date', 'Q5. Start date', 'Q6. Start date'])\n",
    "    print(print_header(\"Responses (Souza et al.)\"), responses_souza.info())\n",
    "\n",
    "def import_responses_barberia():\n",
    "    global responses_barberia\n",
    "    file_name = \"BR BARBERIA et al RPDS COVID policy responses index/CGRT Municipalities.csv\"\n",
    "    file_url = file_path / file_name\n",
    "    responses_barberia = pd.read_csv(file_url, parse_dates=['date'], encoding='utf-8')\n",
    "    print(print_header(\"Responses, Oxford (Barberia et al.)\"), responses_barberia.info())\n",
    "    \n",
    "def import_ibge_codes():\n",
    "    file_path = Path('./OUT/')\n",
    "    global ibge_codes\n",
    "    file_name = \"IBGE GADM city code compat.csv\"\n",
    "    url = file_path / file_name\n",
    "    ibge_codes = pd.read_csv(url, delimiter=\";\", encoding='utf-8', dtype = {'cod_mun_ibge':str,'UF':str},thousands=\".\", engine='c')\n",
    "    ibge_codes.columns = ['uf', 'uf_name', 'ibge_code', 'mun_name', 'GADM_GID_2','pop_2020']\n",
    "    print(print_header(\"IBGE city codes\"),ibge_codes.info())\n",
    "    \n",
    "def import_recoded():\n",
    "    file_path = Path('./OUT/')\n",
    "    global movement_bra_recoded\n",
    "    global movement_bra_by_epi_week\n",
    "    global cases_recoded\n",
    "    global cases_by_epi_week\n",
    "    \n",
    "    #movement range\n",
    "    file_name = 'CVGI_FB_movement_bra_recoded.csv'\n",
    "    url = file_path / file_name\n",
    "    movement_bra_recoded = pd.read_csv(url, encoding=\"utf_8\",parse_dates=['ds'],\n",
    "                                       dtype = {'polygon_id':str, 'epi_week':str, 'all_day_bing_tiles_visited_relative_change':np.float64,\n",
    "                                                'all_day_ratio_single_tile_users':np.float64}, engine='c')\n",
    "    movement_bra_recoded.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    movement_bra_recoded.columns = ['ds', 'GADM_GID_2', 'name_mun','movement_change',\n",
    "                                'stay_put', 'epi_week']\n",
    "    print(print_header(\"Movement Range, recoded\"), movement_bra_recoded.info())\n",
    "    \n",
    "    file_name = 'CVGI_FB_movement_bra_by_epi_week.csv'\n",
    "    url = file_path / file_name\n",
    "    movement_bra_by_epi_week = pd.read_csv(url, encoding=\"utf_8\", dtype = {'polygon_id':str, 'epi_week':str, \n",
    "                                                                           'all_day_bing_tiles_visited_relative_change':np.float64,\n",
    "                                                                           'all_day_ratio_single_tile_users':np.float64, \n",
    "                                                                           'epi_week_col':str}, engine='c')\n",
    "    print(print_header(\"Movement Range, by epidemiological week\"), movement_bra_by_epi_week.info())\n",
    "    \n",
    "    #cases\n",
    "    file_name = 'CVGI_FB_cases_recoded.csv'\n",
    "    url = file_path / file_name\n",
    "    cases_recoded = pd.read_csv(url, encoding=\"utf_8\",parse_dates=['date'],dtype = {'city_ibge_code':str, 'epi_week':str}, engine='c')\n",
    "    print(print_header(\"Cases, recoded\"), cases_recoded.info())\n",
    "    \n",
    "    file_name = 'CVGI_FB_cases_by_epi_week.csv'\n",
    "    url = file_path / file_name\n",
    "    cases_by_epi_week = pd.read_csv(url, encoding=\"utf_8\", dtype = {'city_ibge_code':str, 'epi_week':str, 'acc_cases':int, \n",
    "                                                                    'acc_deaths':int, 'new_cases':int,'new_deaths':int, \n",
    "                                                                    'pop_2019': np.float64, 'epi_week_col':str, 'acc_cases_100k': np.float64,\n",
    "                                                                    'acc_deaths_1mi': np.float64}, engine='c')\n",
    "    cases_by_epi_week.dropna(axis=0, inplace=True, how=\"all\",subset=['city_ibge_code'])\n",
    "    cases_by_epi_week.columns = ['ibge_code', 'epi_week', 'acc_cases', 'acc_deaths', 'new_cases','new_deaths', 'pop_2019',\n",
    "                             'epi_week_col', 'acc_cases_100k','acc_deaths_1mi', 'epi_week_date']\n",
    "    print(print_header(\"Cases, by epidemiological week\"), cases_by_epi_week.info())\n",
    "\n",
    "def import_merged():\n",
    "    file_path = Path('./OUT/')\n",
    "    global movement_bra_merged\n",
    "    global movement_bra_by_epi_week_merged\n",
    "    global cases_merged\n",
    "    global cases_by_epi_week_merged\n",
    "    \n",
    "    #movement range\n",
    "    file_name = 'CVGI_FB_movement_bra_merged.csv'\n",
    "    url = file_path / file_name\n",
    "    movement_bra_merged = pd.read_csv(url, encoding=\"utf_8\",parse_dates=['date'],\n",
    "                                       dtype = {'GADM_GID_2':str, 'epi_week':str, 'movement_change':np.float64,\n",
    "                                                'stay_put':np.float64}, engine='c')\n",
    "    movement_bra_merged.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    print(print_header(\"Movement Range, merged\"), movement_bra_merged.info())\n",
    "    \n",
    "    file_name = 'CVGI_FB_movement_bra_by_epi_week_merged.csv'\n",
    "    url = file_path / file_name\n",
    "    movement_bra_by_epi_week_merged = pd.read_csv(url, encoding=\"utf_8\", \n",
    "                                                  dtype = {'GADM_GID_2':str, 'epi_week':str,'movement_change':np.float64,\n",
    "                                                           'stay_put':np.float64, 'epi_week_col':str}, engine='c')\n",
    "    movement_bra_by_epi_week_merged.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    print(print_header(\"Movement Range, by epidemiological week\"), movement_bra_by_epi_week_merged.info())\n",
    "    \n",
    "    #cases\n",
    "    file_name = 'CVGI_FB_cases_recoded.csv'\n",
    "    url = file_path / file_name\n",
    "    cases_merged = pd.read_csv(url, encoding=\"utf_8\",parse_dates=['date'],dtype = {'city_ibge_code':str, 'epi_week':str}, engine='c')\n",
    "    cases_merged.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    print(print_header(\"Cases, recoded\"), cases_merged.info())\n",
    "    \n",
    "    file_name = 'CVGI_FB_cases_by_epi_week.csv'\n",
    "    url = file_path / file_name\n",
    "    cases_by_epi_week_merged = pd.read_csv(url, encoding=\"utf_8\", \n",
    "                                           dtype = {'city_ibge_code':str, 'epi_week':str, 'acc_cases':int, 'acc_deaths':int, 'new_cases':int,'new_deaths':int, \n",
    "                                                    'pop_2019': np.float64, 'epi_week_col':str, 'acc_cases_100k': np.float64,'acc_deaths_1mi': np.float64}, engine='c')\n",
    "    cases_by_epi_week_merged.dropna(axis=0, inplace=True, how=\"all\",subset=['city_ibge_code'])\n",
    "    \n",
    "    cases_by_epi_week_merged.columns = ['ibge_code', 'epi_week', 'acc_cases', 'acc_deaths', 'new_cases','new_deaths', 'pop_2019',\n",
    "                                        'epi_week_col', 'acc_cases_100k','acc_deaths_1mi']\n",
    "    print(print_header(\"Cases, by epidemiological week\"), cases_by_epi_week_merged.info())\n",
    "\n",
    "def import_cleaned():\n",
    "    file_path = Path('./IN')\n",
    "    global movement_bra_by_epi_week_merged\n",
    "    global cases_by_epi_week_merged\n",
    "    global movement\n",
    "    global cases\n",
    "    global svi_mhdi\n",
    "    file_name = 'CVGI_FB_movement_bra_by_epi_week_merged.csv'\n",
    "    url = file_path / file_name\n",
    "    movement_bra_by_epi_week_merged = pd.read_csv(url, encoding=\"utf_8\", \n",
    "                                                  dtype = {'GADM_GID_2':str, 'epi_week':str,'movement_change':np.float64,\n",
    "                                                           'stay_put':np.float64, 'epi_week_col':str}, engine='c')\n",
    "    movement_bra_by_epi_week_merged.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    movement = movement_bra_by_epi_week_merged\n",
    "    print(print_header(\"Movement Range, by epidemiological week\"), movement.info())\n",
    "    \n",
    "    file_name = 'CVGI_FB_cases_by_epi_week.csv'\n",
    "    url = file_path / file_name\n",
    "    cases_by_epi_week_merged = pd.read_csv(url, encoding=\"utf_8\", \n",
    "                                           dtype = {'city_ibge_code':str, 'epi_week':str, 'acc_cases':int, 'acc_deaths':int, 'new_cases':int,'new_deaths':int, \n",
    "                                                    'pop_2019': np.float64, 'epi_week_col':str, 'acc_cases_100k': np.float64,'acc_deaths_1mi': np.float64}, engine='c')\n",
    "    cases_by_epi_week_merged.dropna(axis=0, inplace=True, how=\"all\",subset=['city_ibge_code'])\n",
    "    \n",
    "    cases_by_epi_week_merged.columns = ['ibge_code', 'epi_week', 'acc_cases', 'acc_deaths', 'new_cases','new_deaths', 'pop_2019',\n",
    "                                        'epi_week_col', 'acc_cases_100k','acc_deaths_1mi']\n",
    "\n",
    "    cases = clean_cases_by_epi_week(cases_by_epi_week_merged)\n",
    "    print(print_header(\"Cases, by epidemiological week\"), cases.info())\n",
    "    \n",
    "    file_name = 'IPEA_BRA_IVS_MHDI_2010 municipalities.xlsx'\n",
    "    url = file_path / file_name\n",
    "    svi_mhdi = pd.read_excel(url, sheet_name='SVI_MHDI', dtype = {'city_name':str, 'UF':str,'ibge_cod':str,'GADM_GID_2':str,\n",
    "                                                           'pop_2020':np.int64,'MHDI_tot':np.float64,\n",
    "                                                           'MHDI_long':np.float64,'MHDI_edu':np.float64,'MHDI_inc':np.float64,\n",
    "                                                           'SVI_tot':np.float64,'SVI_inf':np.float64,'SVI_hum':np.float64,\n",
    "                                                           'SVI_wrk':np.float64,'SVI_quantiles':np.float64}) #, engine='c'\n",
    "    svi_mhdi['reg'] = svi_mhdi.ibge_cod.str[0]\n",
    "    #svi_mhdi.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "    print(print_header(\"IPEA SVI and MHDI data\"), svi_mhdi.info())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Drops</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def drops():\n",
    "    drop_movement()\n",
    "    drop_brasilio()\n",
    "    #create_movement_cases()\n",
    "    #create_cases_case_studies()\n",
    "    #drop_responses_souza()\n",
    "    #drop_responses_barberia()\n",
    "    #drop_gadm_code()\n",
    "    #time_series_clip()\n",
    "\n",
    "def drop_movement():\n",
    "    global movement_bra\n",
    "    #dropping all rows from the Movement Range dataset but Brazilian ones\n",
    "    movement.drop(['baseline_name','baseline_type','polygon_source'],axis='columns', inplace=True)\n",
    "    movement.columns = ['date', 'country', 'GADM_GID_2', 'mun_name','movement_change','stay_put']\n",
    "    movement_bra = movement[movement.country == \"BRA\"]\n",
    "    movement_bra['date'] = pd.to_datetime(movement_bra['date'],format='%Y-%m-%d')\n",
    "    print(print_header(\"Movement Range, dropped\"), movement_bra.info())\n",
    "    \n",
    "    \n",
    "def drop_brasilio():\n",
    "    #dropping all rows from the Brasil.io dataset that are not from municipalities\n",
    "    global cases\n",
    "    cases_temp = cases.loc[pd.notna(cases[\"city\"]),:]\n",
    "    cases_temp['date'] = pd.to_datetime(cases_temp['date'],format='%Y-%m-%d')\n",
    "    cases_temp['city_ibge_code'] = cases_temp['city_ibge_code'].astype('Int64').astype(str)    \n",
    "    #Eliminating entries for states ('city' is null)\n",
    "    cases_temp.dropna(axis=0, inplace=True, how=\"all\",subset=['last_available_confirmed','last_available_confirmed_per_100k_inhabitants', 'last_available_date','last_available_death_rate', 'last_available_deaths', 'order_for_place','place_type', 'state', 'new_confirmed', 'new_deaths'])\n",
    "    #Eliminate unused columns ['order_for_place','place_type','last_available_date']\n",
    "    cases_temp.drop(['order_for_place','place_type','last_available_date','estimated_population','state','last_available_confirmed_per_100k_inhabitants','last_available_death_rate'], axis=1, inplace=True)\n",
    "    cases = cases_temp\n",
    "    cases.columns = ['mun_name', 'ibge_code', 'date', 'epidemiological_week','pop_2019', 'is_last', 'is_repeated','acc_cases', 'acc_deaths', 'new_cases','new_deaths']\n",
    "    print(print_header(\"Cases, dropped\"), cases.info())\n",
    "    \n",
    "def create_movement_cases():\n",
    "    global movement_rj\n",
    "    global movement_sp\n",
    "    global movement_pa\n",
    "    global movement_case_studies\n",
    "    movement_rj = movement_bra[movement_bra.GADM_GID_2 == \"BRA.19.67_1\"]\n",
    "    movement_sp = movement_bra[movement_bra.GADM_GID_2 == \"BRA.25.564_1\"]\n",
    "    movement_pa = movement_bra[movement_bra.GADM_GID_2 == \"BRA.21.305_1\"]\n",
    "    movement_case_studies = pd.concat([movement_rj,movement_sp,movement_pa])\n",
    "    movement_bra.drop(['country'],axis=1,inplace=True)\n",
    "    print(print_header(\"Movement Range, case studies\"), movement_case_studies.info())\n",
    "    \n",
    "def create_cases_case_studies():\n",
    "    global cases_case_studies\n",
    "    global cases_rj\n",
    "    global cases_sp\n",
    "    global cases_pa\n",
    "    cases_rj = cases[cases.ibge_code == '3304557']\n",
    "    cases_sp = cases[cases.ibge_code == '3550308']\n",
    "    cases_pa = cases[cases.ibge_code == '4314902']\n",
    "    cases_case_studies = pd.concat([cases_rj,cases_sp,cases_pa])\n",
    "    print(print_header(\"COVID cases, case studies\"), cases_case_studies.info())\n",
    "    \n",
    "def drop_responses_souza():\n",
    "    global responses_souza\n",
    "    responses_souza.dropna(axis=0, inplace=True, how=\"all\",subset=['Q1. Cordon Sanitaire (monitoring of entrance and exit of people in the municipality)','Q1. Start date','Q2. Restrictive measures to avoid circulation/ agglomeration of people','Q2. Start date','Q3. Measures of social isolation, allowing ONLY essential services','Q3. Start date', 'Q4. Compulsory use of face covers', 'Q4. Start date','Q5. Were any measures implemented to reduce the offer of public transportation?','Q5. What was the percentage of reduction?', 'Q5. Start date','Q6. Were measures of restriction and social isolation eased?','Q6. Start date'])\n",
    "    print(print_header(\"Responses (Souza et al.), dropped\"), responses_souza.info())    \n",
    "    \n",
    "def drop_responses_barberia():\n",
    "    global responses_barberia\n",
    "    responses_barberia[\"city_name\"] = responses_barberia.city + \"/\" + responses_barberia.state\n",
    "    print(print_header(\"Responses (Barberia et al.), dropped\"), responses_barberia.info())    \n",
    "    \n",
    "def drop_gadm_code():\n",
    "    global gadm_code\n",
    "    gadm_code.dropna(axis=1,inplace=True, how=\"all\")\n",
    "    print(print_header(\"GADM Codes, dropped\"), gadm_code.info())    \n",
    "\n",
    "\n",
    "def time_series_clip():\n",
    "    global cases_case_studies\n",
    "    global movement_case_studies\n",
    "    begin_movement = movement_case_studies.ds.min()\n",
    "    end_movement = movement_case_studies.ds.max()\n",
    "    print(\"Begin move: \", begin_movement, \"\\nEnd move: \", end_movement)\n",
    "\n",
    "    begin_cases = cases_case_studies.date.min()\n",
    "    end_cases = cases_case_studies.date.max()\n",
    "    print(\"Begin cases: \", begin_cases, \"\\nEnd cases: \", end_cases)\n",
    "\n",
    "    if begin_movement > begin_cases:\n",
    "        begin_date = begin_movement\n",
    "        begin_early = \"cases_case_studies\" #I need to clip the dataset that has earlier dates\n",
    "\n",
    "    else:\n",
    "        begin_date = begin_cases\n",
    "        begin_early = \"movement_case_studies\"\n",
    "    print(\"Begins earlier:\",begin_early, \"(should be clipped)\")\n",
    "    print(begin_date)\n",
    "\n",
    "    if end_movement < end_cases:\n",
    "        end_date = end_movement\n",
    "        end_later = \"cases_case_studies\"\n",
    "    else:\n",
    "        end_date = end_cases\n",
    "        end_later = \"movement_case_studies\"\n",
    "    print(\"Ends later:\",end_later, \"(should be clipped)\")\n",
    "    print(begin_date)\n",
    "\n",
    "    if begin_early == \"cases_case_studies\":\n",
    "        cases_case_studies = cases_case_studies[cases_case_studies.date>=begin_date]\n",
    "        print(\"cases are clipped at\", cases_case_studies.date.min())\n",
    "    elif begin_early == \"movement_case_studies\":\n",
    "        movement_case_studies = movement_case_studies[movement_case_studies.ds>=begin_date]\n",
    "        print(\"movements are clipped at\", movement_case_studies.ds.min())\n",
    "\n",
    "    if end_later == \"cases_case_studies\":\n",
    "        cases_case_studies = cases_case_studies[cases_case_studies.date<=end_date]\n",
    "        print(\"cases are clipped at\", cases_case_studies.date.max())\n",
    "    elif end_later == \"movement_case_studies\":\n",
    "        movement_case_studies = movement_case_studies[movement_case_studies.ds<=end_date]\n",
    "        print(\"movements are clipped at\", movement_case_studies.ds.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>cleanup</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_movement_bra(rows = None, to_csv = False):\n",
    "    #function takes two parameters:\n",
    "    #rows = a filter for the rows of the dataset, can receive an interval or the \"randSample\" variable\n",
    "    #to_csv = selects whether \"move_bra_week\" is exported to a CSV at the end\n",
    "    global movement_bra\n",
    "    global move_bra_week\n",
    "    if rows == None:\n",
    "        rows =  range(len(movement_bra.index))\n",
    "    movement_bra = new_columns(movement_bra,['epi_week'])\n",
    "    for i in range(len(movement_bra.iloc[rows])):\n",
    "        week_txt = str(movement_bra.ds.iloc[i].isocalendar()[1])\n",
    "        year_txt = str(movement_bra.ds.iloc[i].isocalendar()[0])\n",
    "        if week_txt < 2:\n",
    "            week_txt = \"0\" + week_txt \n",
    "        movement_bra.epi_week.iloc[i] = year_txt + '.' + week_txt\n",
    "    move_bra_week = movement_bra.groupby(['GADM_GID_2','epi_week']).mean()\n",
    "    move_bra_week['epi_week_col'] = move_bra_week.index\n",
    "    for i in range(len(move_bra_week)):\n",
    "        move_bra_week.epi_week_col[i] = move_bra_week.epi_week_col[i][1]\n",
    "    if to_csv == True:\n",
    "        file_path = Path('./OUT/')\n",
    "        file_name = 'CVGI_FB_movement_bra_by_epi_week.csv'\n",
    "        url = file_path / file_name\n",
    "        move_bra_week.to_csv(url, encoding='utf-8-sig',)        \n",
    "    return move_bra_week\n",
    "\n",
    "def cases_cleanup(rows = None, do_merge = False, to_csv = False):\n",
    "    global cases\n",
    "    global cases_week\n",
    "    if rows == None:\n",
    "        rows =  range(len(cases.index))\n",
    "    cases = new_columns(cases,['epi_week'])\n",
    "    #print(rows)    \n",
    "    for i in range(len(cases.iloc[rows])):\n",
    "        week_txt = right(str(cases.epidemiological_week.iloc[i]),2) #Brasil.io always codes \"epidemiological_week\" with 6 digits\n",
    "        year_txt = left(str(cases.epidemiological_week.iloc[i]),4)\n",
    "        cases.epi_week.iloc[i] = year_txt + \".\" + week_txt\n",
    "    if do_merge == False:\n",
    "        return cases\n",
    "    elif do_merge == True:\n",
    "        #we'll have two new datasets: one for the variables that are summed while groupping\n",
    "        #another for the variables that are averaged out while grouping\n",
    "        #sum temp dataset, removes columns we'll not use\n",
    "        #we are left with the columns: 'new_confirmed', 'new_deaths', 'acc_cases', 'acc_deaths'\n",
    "        cases_temp_sum = cases[['ibge_code','epi_week','new_confirmed','new_deaths','acc_cases','acc_deaths']]\n",
    "        cases_week_sum = cases_temp_sum.groupby(['ibge_code','epi_week']).sum()\n",
    "        #mean temp dataset, we just keep one column, makes sense to slice \n",
    "        cases_temp_mean = cases[[\"ibge_code\",\"epi_week\",\"pop_2019\"]]\n",
    "        cases_week_mean = cases_temp_mean.groupby(['ibge_code','epi_week']).mean()\n",
    "\n",
    "        #merging the datasets\n",
    "        cases_week = cases_week_sum.merge(cases_week_mean, how='left',left_index=True,right_index=True)\n",
    "        cases_week['epi_week_col'] = cases_week.index\n",
    "        for i in range(len(cases_week)):\n",
    "            cases_week.epi_week_col[i] = cases_week.epi_week_col[i][1]\n",
    "        \n",
    "        #calculating rates of cases per 100k people and deaths per million\n",
    "        cases_week['acc_cases_100k'] = round(cases_week.acc_cases / (cases_week.pop_2019 / 100000),2)\n",
    "        cases_week['acc_deaths_1mi'] = round(cases_week.acc_deaths / (cases_week.pop_2019 / 1000000),2)\n",
    "        \n",
    "    if to_csv == True:\n",
    "        file_path = Path('./OUT/')\n",
    "        file_name = 'CVGI_FB_cases_by_epi_week.csv'\n",
    "        url = file_path / file_name\n",
    "        cases_week.to_csv(url, encoding='utf-8-sig')        \n",
    "    return cases_week\n",
    "\n",
    "def clean_cases_by_epi_week(df):\n",
    "    #global cases_by_epi_week\n",
    "    df['epi_week_date'] = \".0\"\n",
    "    df['epi_week_date'] = df.epi_week.astype(\"str\").str.pad(7, side='right', fillchar='0') + df.epi_week_date\n",
    "    df['epi_week_date'] = pd.to_datetime(df['epi_week_date'],format='%Y.%W.%w')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibge_uf_dict = {11:\"RO\",12:\"AC\",13:\"AM\",14:\"RR\",15:\"PA\",16:\"AP\",17:\"TO\",\n",
    "                21:\"MA\",22:\"PI\",23:\"CE\",24:\"RN\",25:\"PB\",26:\"PE\",27:\"AL\",28:\"SE\",29:\"BA\",\n",
    "                31:\"MG\",32:\"ES\",33:\"RJ\",35:\"SP\",\n",
    "                41:\"PR\",42:\"SC\",43:\"RS\",\n",
    "                50:\"MS\",51:\"MT\",52:\"GO\",53:\"DF\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Kaplan-Meier plot and upscaling functions</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upscaling function city level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_cities(cases):\n",
    "    start_time = time.time()\n",
    "    total_deaths = 0\n",
    "    for id_wk, val_wk in enumerate(cases.epi_week):\n",
    "        #check if there are any cases, otherwise skip\n",
    "        var_deaths = cases[(cases.epi_week == val_wk)].new_deaths.tolist()[0]\n",
    "        #check if the death number of a city is positive. we are ignoring the negative numbers.\n",
    "        if var_deaths > 0: total_deaths = total_deaths + var_deaths\n",
    "    events = pd.DataFrame({'event': pd.Series([], dtype='int'),'week':pd.Series([], dtype='int')}, index=range(total_deaths))\n",
    "    c = 0\n",
    "    for id_wk, val_wk in enumerate(cases.epi_week):\n",
    "        #check if there are any cases, otherwise skip\n",
    "        var_deaths = cases[(cases.epi_week == val_wk)].new_deaths.tolist()[0]\n",
    "        #ignore negative death numbers (they are low, and apply to very few cases, see if this needs to be improved)\n",
    "        if var_deaths > 0:\n",
    "            for i in range(var_deaths):\n",
    "                events['event'].iloc[c] = 1 \n",
    "                events['week'].iloc[c] = id_wk\n",
    "                c = c + 1\n",
    "    t1 = \"--- %s seconds ---\" % (time.time() - start_time)\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upscaling for multiple cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_mc(cities_list, dates=None):\n",
    "    #cases_uf = cases[(cases['ibge_code'].str.startswith(uf)) & (cases['epi_week_date'] < ('2021-02-25'))]\n",
    "    unique_cities = cities_list\n",
    "    dict_events_mc = {}\n",
    "    \n",
    "    if dates == None:\n",
    "        for ibge_code in unique_cities:\n",
    "            #print(ibge_code)\n",
    "            cases_test = cases[cases.ibge_code == ibge_code][['epi_week','new_deaths']]\n",
    "            dict_events_mc[ibge_code] = events_cities(cases_test)\n",
    "    #function returns a dict, it can be transformed in a dataframe using concat\n",
    "    else:\n",
    "        for ibge_code in unique_cities:\n",
    "            #print(ibge_code)\n",
    "            cases_test = cases[(cases.ibge_code == ibge_code)&(cases.epi_week_date < dates)][['epi_week','new_deaths']]\n",
    "            dict_events_mc[ibge_code] = events_cities(cases_test)\n",
    "    df_events = pd.concat(dict_events_mc)\n",
    "    return df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upscaling function state level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def events_uf(uf = '11'):\n",
    "    #uf is a string! don't forget\n",
    "    cases_uf = cases[(cases['ibge_code'].str.startswith(uf)) & (cases['epi_week_date'] < ('2021-02-25'))]\n",
    "    unique_cities = cases_uf.ibge_code.unique()\n",
    "    #print(unique_cities)\n",
    "    dict_events_uf = {}\n",
    "    \n",
    "    for ibge_code in unique_cities:\n",
    "        #print(ibge_code)\n",
    "        cases_test = cases[cases.ibge_code == ibge_code][['epi_week','new_deaths']]\n",
    "        dict_events_uf[ibge_code] = events_cities(cases_test)\n",
    "    #function returns a dict, it can be transformed in a dataframe using concat\n",
    "    df_events = pd.concat(dict_events_uf)\n",
    "    return df_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaplan-Meier plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kmf(dataframe,cities):\n",
    "    kmf = KaplanMeierFitter() \n",
    "    df = dataframe\n",
    "    #list currently takes a max of 5 cities because of the ax elements in the plot at the end\n",
    "    cities_list = cities\n",
    "\n",
    "    T = df['week']     ## time to event\n",
    "    E = df['event']      ## event occurred or censored\n",
    "\n",
    "\n",
    "    #creating a the cohorts. this takes a list of index (named) keys\n",
    "    groups = df.loc[cities_list]\n",
    "    #create an empty series object to receive the dataframe for each city below\n",
    "    ix = pd.Series([])\n",
    "    for i in range(len(cities_list)):\n",
    "        #create a temporary dataframe, holding the events for each city\n",
    "        ix[i] = groups.loc[cities_list[i]].index\n",
    "        cities_label = cities_list[i]\n",
    "        #cities_label = ibge_codes[ibge_codes.ibge_code == cities_list[i]].mun_name\n",
    "        #passing on the dataframe's time and event columns to the Kaplan-Meier function, and the index keys as labels (we can search the IBGE database for its name later)\n",
    "        kmf.fit(T[ix[i]], E[ix[i]], label=cities_label) \n",
    "        figsize(14,7)\n",
    "        if i == 0:\n",
    "            ax = kmf.plot()\n",
    "        elif i > 0:\n",
    "            ax1 = kmf.plot(ax=ax)\n",
    "\n",
    "    plt.show()\n",
    "    return kmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
